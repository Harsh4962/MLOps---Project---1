# ğŸš— Vehicle Insurance ML Pipeline â€“ Production-Ready MLOps Project

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10-blue.svg">
  <img src="https://img.shields.io/badge/Framework-FastAPI-green.svg">
  <img src="https://img.shields.io/badge/Cloud-AWS-orange.svg">
  <img src="https://img.shields.io/badge/MLOps-CI%2FCD%2C%20Docker%2C%20MongoDB-success">
</p>

> **A full-stack, production-grade Machine Learning pipeline for Vehicle Insurance data â€“ built with Clean Code, Cloud Services, Docker, CI/CD and AWS S3 integration.**

---

## ğŸŒŸ Highlights

âœ… Professional project structure using Python packaging best practices

âœ… End-to-end pipeline: Data ingestion â†’ Validation â†’ Transformation â†’ Training â†’ Evaluation â†’ Deployment

âœ… MongoDB Atlas used as a scalable NoSQL data storage

âœ… AWS S3 integration for model registry

âœ… Containerized using Docker, deployed via GitHub Actions to AWS EC2

âœ… CI/CD pipeline for seamless automation and delivery

âœ… User-friendly prediction interface with FastAPI

âœ… Self-hosted GitHub Runner on EC2 instance for scalable builds

---

## ğŸš€ Project Setup

### ğŸ§± 1. Template & Virtual Environment

```bash
# Create file structure
python template.py

# Setup environment
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
pip list  # Verify installations
```

> ğŸ“Œ **Local imports** handled via `setup.py` and `pyproject.toml`. Learn more inside [`crashcourse.txt`](./crashcourse.txt).

---

## ğŸŒ MongoDB Atlas Setup

1. Signup at [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)
2. Create new project â†’ create M0 cluster â†’ setup DB user & password
3. Add IP `0.0.0.0/0` in **Network Access**
4. Copy Python connection string and securely store it (replace `<password>`)
5. Create `notebook/mongoDB_demo.ipynb` â†’ use `vehicle` kernel
6. Load dataset & push to MongoDB using PyMongo
7. Confirm upload from Atlas Dashboard â†’ Collections view

---

## âš™ï¸ Logging, Exceptions & Notebooks

* âœï¸ Modular logging system: `logger.py`
* âŒ Custom exceptions for debugging: `exception.py`
* ğŸ“Š Exploratory Data Analysis and Feature Engineering notebooks provided

---

## ğŸ“¥ Data Ingestion Pipeline

* Define schema in `constants/__init__.py`
* Setup MongoDB connection in `configuration/mongo_db_connection.py`
* Fetch and convert data to DataFrame via `data_access/proj1_data.py`
* Create `DataIngestionConfig` and `DataIngestionArtifact` classes
* Pipeline logic in `components/data_ingestion.py`
* Trigger via `demo.py` with Mongo URL configured as:

### ğŸ” Set Mongo URL in ENV

**Mac/Linux (Bash):**

```bash
export MONGODB_URL="mongodb+srv://<username>:<password>..."
```

**Windows (PowerShell):**

```powershell
$env:MONGODB_URL="mongodb+srv://<username>:<password>..."
```

âœ… Add `artifact/` to `.gitignore`

---

## ğŸ§ª Data Validation, Transformation & Model Training

* Define schema in `config/schema.yaml`
* Core logic in `utils/main_utils.py`
* Implement Data Validation, Transformation, and Training as modular components
* Add `estimator.py` to support transformations & modeling

---

## â˜ï¸ AWS Setup for Model Evaluation & Registry

### ğŸ”‘ IAM & S3 Setup

1. Login to AWS â†’ Region: `us-east-1`
2. IAM â†’ Create user `firstproj` with `AdministratorAccess`
3. Download Access Keys CSV
4. Export keys as ENV vars:

```bash
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."
```

5. Create S3 bucket:

   * Name: `my-model-mlopsproj`
   * Uncheck â€œBlock all public accessâ€

6. Define in `constants/__init__.py`:

```python
MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE = 0.02
MODEL_BUCKET_NAME = "my-model-mlopsproj"
MODEL_PUSHER_S3_KEY = "model-registry"
```

7. Add logic to `src/aws_storage/` and `entity/s3_estimator.py`

---

## ğŸ“Š Model Evaluation & Pushing

* Evaluate model drift
* Push model artifacts to S3
* Deploy latest version with version tracking

---

## ğŸ§  Prediction Pipeline

* Add `app.py` to serve FastAPI-based inference
* Add UI resources to `static/` and `templates/`

---

## âš™ï¸ CI/CD + Docker + GitHub Actions

### ğŸ³ Docker Setup

* Create `Dockerfile` and `.dockerignore`
* Add GitHub Workflow YAML: `.github/workflows/aws.yaml`

### ğŸ” GitHub Secrets

Add the following secrets in GitHub â†’ Settings â†’ Actions:

* `AWS_ACCESS_KEY_ID`
* `AWS_SECRET_ACCESS_KEY`
* `AWS_DEFAULT_REGION`
* `ECR_REPO`

### ğŸ—ï¸ Self-Hosted Runner (EC2)

1. Launch EC2 Ubuntu instance (T2 Medium)
2. SSH into EC2 â†’ install Docker:

```bash
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
```

3. Connect GitHub Runner:

   * Settings â†’ Actions â†’ New self-hosted runner
   * Follow Linux commands from GitHub on EC2

4. Run `./run.sh` to activate runner

---

## ğŸŒ Deploy to EC2

1. Create Inbound Rule in EC2:

   * Port: `5080` â†’ IP: `0.0.0.0/0`
2. Visit `http://<EC2_PUBLIC_IP>:5080` to launch the app
3. Access `/training` route to retrain model

---

## ğŸ“ Directory Structure

```
ğŸ“¦Vehicle-ML-Pipeline
 â”£ ğŸ“‚src
 â”ƒ â”£ ğŸ“‚components
 â”ƒ â”£ ğŸ“‚data_access
 â”ƒ â”£ ğŸ“‚entity
 â”ƒ â”£ ğŸ“‚configuration
 â”ƒ â”£ ğŸ“‚aws_storage
 â”ƒ â”£ ğŸ“‚utils
 â”£ ğŸ“‚templates
 â”£ ğŸ“‚static
 â”£ ğŸ“‚notebook
 â”£ ğŸ“œDockerfile
 â”£ ğŸ“œapp.py
 â”£ ğŸ“œsetup.py
 â”£ ğŸ“œpyproject.toml
 â”£ ğŸ“œrequirements.txt
 â”£ ğŸ“œREADME.md
 â”— ğŸ“‚.github/workflows
```

---

## ğŸ‘¨â€ğŸ’» Author

**Harshwardhan Ghongade**
ğŸš€ Passionate about ML, Deep Learning & MLOps
ğŸ“ Undergraduate at IIT Kanpur



